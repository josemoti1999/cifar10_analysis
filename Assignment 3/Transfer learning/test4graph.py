
import matplotlib.pyplot as plt
list_x=[]
for i in range (70):
  list_x.append(i+1)
  
  
loss_train=[2.1555333285380507, 1.4732821320023988, 1.3676578618986222, 1.3152711940238544, 1.2510963268292226, 1.3018255386206194, 1.362938519786386, 1.3886192928799583, 1.2984009481147123, 1.338261217869761, 1.3628961725917923, 1.322635224713084, 1.3216373816780422, 1.3714331440303638, 1.355357514622876, 1.3781371749270603, 1.3902721624545125, 1.2794922292994722, 1.292375248487648, 1.4003205698774295, 1.2573395130579428, 1.3389081694281009, 1.2492376695508542, 1.26630991262853, 1.299489178163621, 1.3142213736043866, 1.295847072320826, 1.2783699604251502, 1.3473846599878863, 1.3396124894661672]
accuracy_train=[70.272, 73.0, 74.262, 74.454, 74.614, 74.096, 74.15, 74.198, 74.808, 74.53, 73.976, 74.75, 74.208, 74.212, 74.13, 74.12, 74.268, 74.664, 74.424, 73.888, 74.95, 74.034, 74.818, 74.122, 74.516, 74.34, 74.654, 74.524, 74.186, 74.372]
accuracy_test=[68.42, 75.23, 70.01, 73.42, 74.7, 70.84, 73.27, 71.84, 69.69, 74.29, 72.83, 72.06, 70.74, 73.54, 73.83, 74.44, 73.67, 75.98, 70.82, 76.23, 70.1, 74.12, 76.04, 71.06, 76.35, 73.21, 74.82, 70.02, 74.27, 75.14]
loss_test=[1.9307720625400544, 1.3181846553087235, 1.6650333428382873, 1.3773411011695862, 1.2666567939519882, 1.6716393363475799, 1.4285269278287887, 1.60693834066391, 1.8366602408885955, 1.2242735373973845, 1.490334951877594, 1.406152655482292, 1.918024287223816, 1.4553119206428529, 1.32630890250206, 1.3898584204912185, 1.4044938665628433, 1.1447072410583496, 1.6277104914188385, 1.2367174324393273, 1.590197949409485, 1.3481242740154267, 1.0863572078943253, 1.5417859381437302, 1.1173646333813667, 1.4547035163640976, 1.2613203209638595, 1.6179457807540893, 1.4430983293056487, 1.2683214747905731]


loss_train=loss_train+[0.7362569944404275, 0.6178988063579325, 0.5670982714351791, 0.5420677589486017, 0.5281088017594174, 0.5159049252491168, 0.5121447057141673, 0.5058364398644098, 0.5022723239553554, 0.5032213746434282, 0.5002878745803443, 0.5003764418613575, 0.5035187268196164, 0.4951231917914222, 0.5005437304906528, 0.49856053227963654, 0.4945704309684236, 0.49613897788250233, 0.4982921619092107, 0.49350808297886567]
accuracy_train=accuracy_train+[81.434, 82.064, 82.256, 82.402, 82.404, 82.446, 82.59, 82.53, 82.554, 82.378, 82.432, 82.558, 82.314, 82.614, 82.528, 82.57, 82.686, 82.576, 82.598, 82.768]
accuracy_test=accuracy_test+[79.73, 79.46, 79.5, 79.93, 79.58, 79.82, 78.85, 79.73, 80.35, 79.81, 80.38, 80.31, 80.02, 80.07, 79.75, 79.61, 80.2, 79.49, 79.73, 80.31]
loss_test=loss_test+[0.8037854808568955, 0.7375472187995911, 0.6939145591855049, 0.661880937218666, 0.6515950992703438, 0.6340740570425987, 0.6470265632867813, 0.6203573378920555, 0.6001086813211441, 0.6099525409936905, 0.5850276574492455, 0.594634185731411, 0.6036062318086625, 0.5966579660773277, 0.5923595169186592, 0.6053161916136741, 0.5930739739537239, 0.6010468602180481, 0.5937738180160522, 0.5799758130311966]


loss_train=loss_train+[0.46509946993244883, 0.4637143021959173, 0.46317134214484174, 0.4630116325662569, 0.4627768202206058, 0.4622643388750608, 0.4626103763080314, 0.462014839053154, 0.46283778334822495, 0.46241647096546107, 0.4624633194540468, 0.4621984645762407, 0.462848588023954, 0.46266238349477956, 0.46227647428927215, 0.46249099750348066, 0.4620498609359917, 0.46268816700067056, 0.462400526265659, 0.46250664379895495]
accuracy_train=accuracy_train+[83.806, 83.772, 83.768, 83.922, 83.854, 83.822, 83.84, 83.77, 83.834, 83.862, 83.918, 83.886, 83.87, 83.824, 83.834, 83.828, 83.956, 83.872, 83.876, 83.886]
accuracy_test=accuracy_test+[80.95, 80.97, 80.76, 81.07, 80.99, 80.91, 81.08, 80.96, 81.01, 80.97, 81.05, 80.94, 81.01, 81.06, 81.04, 80.94, 80.89, 81.0, 81.04, 81.08]
loss_test=loss_test+[0.5650598394870758, 0.5631702595949173, 0.5655985167622566, 0.5627618938684463, 0.5632535251975059, 0.5619952195882797, 0.5620155230164527, 0.5638623839616775, 0.5606146025657653, 0.5617894107103347, 0.5610733446478844, 0.5616715642809867, 0.5608776524662972, 0.5621043759584426, 0.5599858391284943, 0.560528134405613, 0.5613212749361992, 0.5592392855882644, 0.5603584703803063, 0.5615109661221505]





plt.figure(1)
plt.plot(list_x,loss_train,'-g')
plt.title('Test 4')
plt.xlabel('epochs')
plt.ylabel('Train set loss')
plt.legend(loc='best')
plt.savefig('test_4_train_loss')

plt.figure(2)
plt.ylim(0,105)
plt.plot(list_x,accuracy_train,'-g')
plt.title('Test 4')
plt.xlabel('epochs')
plt.ylabel('Train set Accuracy')
plt.legend(loc='best')
plt.savefig('test_4_train_accuracy')


plt.figure(3)
plt.ylim(0,105)
plt.plot(list_x,accuracy_test,'-g')
plt.title('Test 4')
plt.xlabel('epochs')
plt.ylabel('Test set Accuracy')
plt.legend(loc='best')
plt.savefig('test_4_test_accuracy')




plt.figure(4)
plt.plot(list_x,loss_test,'-g')
plt.title('Test 4')
plt.xlabel('epochs')
plt.ylabel('Test set Loss')
plt.legend(loc='best')
plt.savefig('test_4_test_loss')